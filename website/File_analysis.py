from flask import Blueprint, render_template, request, flash, jsonify, send_from_directory,  Response, send_file
from langdetect import detect
from weasyprint import HTML
from datetime import datetime
from io import StringIO
from flask import session
from flask import redirect
from werkzeug.utils import secure_filename
from collections import Counter
from flask import current_app
import chardet
import shutil
import tempfile
from html2image import Html2Image
import re
import os
import time
from PIL import Image, ImageDraw, ImageFont
import random
from sentiment_analyser import SentimentAnalyser
from Summariser import run_summarizer
from word_cloud_generator import WordCloudGenerator
from Keyword_collocation import KWICAnalyser
from checklanguage import LanguageChecker
import pandas as pd
import plotly.express as px
import threading
from os.path import splitext, basename
import string
import shutil
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from .models import Feedback
from . import db
en_stopwords = list(stopwords.words('english'))
cy_stopwords = open('/freetxt/website/data/welsh_stopwords.txt', 'r', encoding='iso-8859-1').read().split('\n') # replaced 'utf8' with 'iso-8859-1'
STOPWORDS = set(en_stopwords + cy_stopwords+ ["a", "an", "the", "and", "or", "in", "of", "to", "is", "it", "that", "on", "was", "for", "as", "with", "by"])

PUNCS = string.punctuation

PUNCS += '''!â†’()-[]{};:"\,<>?@#$%^&*_~'''
# Initialize a lock
file_lock = threading.Lock()

FileAnalysis = Blueprint('fileanalysis', __name__)

#### utilities
ALLOWED_EXTENSIONS = {'txt', 'csv', 'xls', 'xlsx','tsv'}
import os
os.environ["TRANSFORMERS_CACHE"] = "/freetxt/huggingface_cache"


from PIL import Image
logo_path = "/freetxt/website/static/images/logo.png"
def append_logo_to_image(image_path, logo_path, output_path):
    # Open the main image and the logo
    main = Image.open(image_path)
    logo = Image.open(logo_path)

    # Resize the logo if needed
    # Here, we're resizing the logo to have the same width as the main image, 
    # while maintaining its aspect ratio
    logo_width = main.width
    logo_height = int((logo_width / logo.width) * logo.height)
    logo = logo.resize((logo_width, logo_height))

    # Create a new image with the combined height of the main image and the logo
    combined = Image.new('RGB', (main.width, main.height + logo_height))
    combined.paste(main, (0, 0))
    combined.paste(logo, (0, main.height))

    # Save the resulting image
    combined.save(output_path)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def sanitize_column_name(name):
    # This function will keep only alphanumeric characters and spaces, then replace spaces with underscores.
    sanitized = re.sub(r'[^a-zA-Z0-9 ]', '', name)
    return sanitized.replace(' ', '_')
date_formats = [
    '%Y-%m-%d',    # Year-Month-Day with dashes
    '%d/%m/%Y',    # Day/Month/Year with slashes
    '%m/%d/%Y',    # Month/Day/Year with slashes
    '%d-%m-%Y',    # Day-Month-Year with dashes
    '%d.%m.%Y',    # Day.Month.Year with dots
    '%d/%m/%y',    # Day/Month/Year with slashes and two-digit year
    '%m-%d-%Y',    # Month-Day-Year with dashes
    '%m.%d.%Y',    # Month.Day.Year with dots
    '%m/%d/%y',    # Month/Day/Year with slashes and two-digit year
    '%Y.%m.%d',    # Year.Month.Day with dots
    '%Y/%m/%d',    # Year/Month/Day with slashes
    '%y-%m-%d',    # Two-digit Year-Month-Day with dashes
    '%d %B %Y',    # Day full-month-name Year
    '%d %b %Y',    # Day abbreviated-month-name Year
    '%Y-%m-%d %H:%M:%S',  # Date and time with seconds
    '%Y-%m-%dT%H:%M:%SZ', # ISO 8601 format
    '%d-%b-%Y',    # Day abbreviated-month-name Year with dashes
    '%Y%m%d'       # Continuous digits for YearMonthDay
]

def convert_date_columns(df, date_formats):
    for col in df.columns:
        for fmt in date_formats:
            try:
                df[col] = pd.to_datetime(df[col], format=fmt, errors='raise')
                df[col] = df[col].dt.strftime('%d/%m/%Y')
                break
            except Exception:
                continue
    return df

def read_file(file_path):
    if file_path.endswith('.csv'):
            try:
                data = pd.read_csv(file_path)
            except UnicodeDecodeError:
                try:
                    data = pd.read_csv(file_path, encoding='ISO-8859-1')
                except:
                    data = pd.read_csv(file_path, encoding='utf-8', errors='replace')

    elif file_path.endswith('.tsv'):
        try:
            data = pd.read_csv(file_path, delimiter='\t')
        except UnicodeDecodeError:
            try:
                data = pd.read_csv(file_path, delimiter='\t', encoding='ISO-8859-1')
            except:
                data = pd.read_csv(file_path, delimiter='\t', encoding='utf-8', errors='replace')
    elif file_path.endswith('.xlsx'):
        data = pd.read_excel(file_path, sheet_name=0)
    elif file_path.endswith('.txt'):
        # Guess the encoding of the file
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        encoding = result['encoding']

        with open(file_path, 'r', encoding=encoding, errors='replace') as f:
            lines = f.readlines()
            data = pd.DataFrame(lines, columns=['Reviews'])
    else:
        data = pd.DataFrame()
    
    data.columns = [sanitize_column_name(col) for col in data.columns]
    data = convert_date_columns(data,date_formats)
    return data
  # remove the filepath from the session

def cleanup_expired_sessions():
    filepath = session.get('uploaded_file_path')
    # Path where you want to check for file existence
    specific_path = "/freetxt/website/static/example-data-hub/"
    
    # Check if the file exists in the specific directory
    file_in_specific_path = os.path.join(specific_path, os.path.basename(filepath))
    
    if os.path.exists(file_in_specific_path):
        return  # exit the function as the file exists in the specified directory

    # Check if the session is expired
    
    elif 'expiration_time' in session and session['expiration_time'] <= time.time():
        os.remove(filepath)
        session.pop('uploaded_file_path', None)
    session.clear()
@FileAnalysis.route('/fileanalysis', methods=['GET', 'POST'])
def fileanalysis():
    sentences = []
    if request.method == 'POST':
        input_method = request.form.get('input-method') or request.json.get('input_method')
      
        if input_method == 'text':
           
            text = request.form.get('text-to-analyze')
            raw_sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
            #sentences = [{"Sentences": sentence} for sentence in raw_sentences]
            sentences = [ sentence for sentence in raw_sentences]
            #print(sentences)
        elif input_method == 'example':
            example_file = request.form.get('example-data')
            if not example_file:
            # Handle the error - maybe raise an exception or return an error response.
                raise ValueError("The example_file is not provided or is None")

            file_path = os.path.join('/freetxt/website/static/example-data-hub', example_file)
            file_extension = os.path.splitext(file_path)[1].lower()  # Extract the file extension

            # Differentiate the behavior based on the file extension
            if file_extension == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    # Filter out empty lines and strip whitespace
                    sentences = [line.strip() for line in lines if line.strip()]
                    # Convert the list of sentences to a pandas DataFrame
                    data = pd.DataFrame(sentences, columns=['review'])
            elif file_extension in ['.csv', '.xlsx', '.tsv']:
                if file_extension == '.csv':
                    data = pd.read_csv(file_path)
                elif file_extension == '.tsv':
                    try:
                        # First try utf-8 encoding (the default)
                        data = pd.read_csv(file_path, delimiter='\t')
                    except UnicodeDecodeError:
                        try:
                            # If utf-8 fails, try ISO-8859-1
                            data = pd.read_csv(file_path, delimiter='\t', encoding='ISO-8859-1')
                        except:
                            # As a last resort, replace invalid characters
                            data = pd.read_csv(file_path, delimiter='\t', encoding='utf-8', errors='replace')

                else:  # for .xlsx
                    data = pd.read_excel(file_path)
            else:
                raise ValueError("Unsupported file type!")
        
        
            session['uploaded_file_path'] = file_path  # save the file path to session
            session['uploaded_file_name'] = example_file
            session['data'] = data.to_json()  # Convert the DataFrame to JSON and store in session
            return jsonify({"columns": list(data.columns)})
            
        
        elif input_method == 'upload':
        # Before saving the new file path, let's clean up any previously uploaded file
            previous_file_path = session.get('uploaded_file_path')
            
            if previous_file_path and os.path.exists(previous_file_path):
               # os.remove(previous_file_path)
                #print(previous_file_path)
               # session.pop('uploaded_file_path', None)  # remove the old filepath from the session
               # session.pop('data', None)  # clear the old data from the session
               cleanup_expired_sessions()
            if 'file' not in request.files:
                flash('No file part')
                return redirect(request.url)
    
            file = request.files['file']
            if file.filename == '':
                flash('No selected file')
                return redirect(request.url)
    
            if file and allowed_file(file.filename):
                filename = secure_filename(file.filename)
                filepath = os.path.join(current_app.config['UPLOAD_FOLDER'], filename)
                #print(filepath)
                file.save(filepath)
                session['uploaded_file_path'] = filepath  # save the new file path to session
                session['uploaded_file_name'] = filename 
                data = read_file(filepath)
                session['data'] = data.to_json()  # Convert the DataFrame to JSON and store in new session
                return jsonify({"columns": list(data.columns)})
            
            else:
                flash('Invalid file type')
                return redirect(request.url)

        elif input_method == 'columns':
            selected_columns = request.json.get('selectedColumns')
    
            data_json = session.get('data')
            if not data_json:
                return jsonify({"message": "No data found in session", "data": []})

            data = pd.read_json(data_json)  # Convert the JSON back to DataFrame
            #for col in data.columns:
            # Check if the first non-NaN value in the column looks like a date
                # sample_val = data[col].dropna().iloc[0] if not data[col].dropna().empty else None
                # if sample_val and isinstance(sample_val, str) and "GMT" in sample_val:
                    # Step 3: Convert the column to the desired date format
                  #   data[col] = pd.to_datetime(data[col], format='%d-%m-%Y', errors='coerce')

            data=convert_date_columns(data, date_formats)
            extracted_data = data[selected_columns].dropna().drop_duplicates().to_dict(orient='records')
            session['extracted_data'] = pd.DataFrame(extracted_data).to_json()
            return jsonify({"message": "Data extracted", "data": extracted_data})
           
        cleanup_expired_sessions()
        return jsonify({"sentences": sentences})

    return render_template('Fileanalysis.html')

@FileAnalysis.route('/process_sentences', methods=['GET', 'POST'])
def handle_selected_sentences():
    data = request.get_json()
    selected_sentences = data.get('sentences', [])
    language = data.get('language', 'en')   
    #search_word = data.get('search_word', 'see')
    # Get word frequencies
    words = []
    for sentence in selected_sentences:
        extracted_words = re.findall(r'\w+', sentence.lower())
        filtered_words = [word for word in extracted_words if word not in STOPWORDS and word not in PUNCS]
        words.extend(filtered_words)

            # Counting frequencies
    word_frequencies = dict(Counter(words))
    # Sorting by highest frequency
    sorted_word_frequencies = dict(sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True))

    sentiment_analyser = SentimentAnalyser()
    # Get sentiment analysis results
    sentiment_data, sentiment_counts, pie_chart_html,bar_chart_html = sentiment_analysis(selected_sentences,language)
    df_sentiment= pd.DataFrame(sentiment_data)
    with file_lock:
        scatter_text_html = sentiment_analyser.generate_scattertext_visualization(df_sentiment,language)
    ## Word Tree Generator
    most_frequent_word = next(iter(sorted_word_frequencies))
    search_word = most_frequent_word
    #print (search_word)
    sentences = [[s] for s in selected_sentences]

    wordTreeData = sentences  # This should be a list
    summary = summarize_text(selected_sentences)
    #print(summary)
    return jsonify({
        "status": "success", 
        "wordFrequencies": sorted_word_frequencies, 
        "sentimentData": sentiment_data, 
        "sentimentCounts": sentiment_counts,
        'sentimentPlotPie': pie_chart_html,
        'sentimentPlotBar': bar_chart_html,
        "wordTreeData": wordTreeData, "search_word": search_word, "summary": summary,  "scatterTextHtml": scatter_text_html
    })

def sentiment_analysis(sentences,language, sentiment_classes=3):
    analyser = SentimentAnalyser()
    results = analyser.analyse_sentiment(sentences,language,sentiment_classes)
    if results:
        sentiments, sentiment_counts = results
        dfanalysis = pd.DataFrame(sentiments, columns=['Review', 'Sentiment Label', 'Confidence Score'])
        data = dfanalysis.to_dict(orient='records')
        color_map = {
    "Very negative": "#ff3333",  
    "Negative": "#ff8a3d",     
    "Neutral": "#b0b0b0",      
    "Positive": "#c5e17a" ,     
    "Very positive": "#6ebd45" ,
    "Negyddol Iawn": "#ff3333",  
    "Negyddol": "#ff8a3d",     
    "Niwtral": "#b0b0b0",      
    "Cadarnhaol": "#c5e17a" ,     
    "Cadarnhaol Iawn": "#6ebd45" 
}
        ## # Determine which text to use based on the selected language
        visualisation_text_en = "Visualisation by"
        visualisation_text_cy = "Gweledigaeth gan"
        if language == 'en':
            visualisation_text = visualisation_text_en
        elif language == 'cy':
             visualisation_text = visualisation_text_cy
        # Generating the pie chart
        if language == 'en':
        
            fig = px.pie(values=sentiment_counts.values(), names=sentiment_counts.keys(),
                 title='Sentiment Distribution', color=sentiment_counts.keys(),
                 color_discrete_map=color_map)
        elif language == 'cy':
            fig = px.pie(values=sentiment_counts.values(), names=sentiment_counts.keys(),
                 title='Dosbarthiad Sentiment', color=sentiment_counts.keys(),
                 color_discrete_map=color_map)
        plot_html_pie = fig.to_html(full_html=False)
        fig.write_image("/freetxt/website/static/Sentiment_plots/sentiment_pie.png") 
        fig.write_html("/freetxt/website/static/Sentiment_plots/sentiment_pie.html")
        with open("/freetxt/website/static/Sentiment_plots/sentiment_pie.html", "r", encoding="utf-8") as f:
            content = f.read()

        # Add the "Visualisation by" text and logo image at the bottom
        addition = f"""
    <div style="text-align:center; margin-top:30px;">
        {visualisation_text} <img src="https://ucrel-freetxt-2.lancs.ac.uk/static/images/logo.png" alt="Logo" style="height:40px;">
    </div>
    """

        # Append the new content just before the closing body tag
        content = content.replace("</body>", addition + "\n</body>")

        # Write the updated content back to the file
        with open("/freetxt/website/static/Sentiment_plots/sentiment_pie.html", "w", encoding="utf-8") as f:
            f.write(content)

        # Generating the bar chart
        
        if language == 'en':
            fig_bar = px.bar(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()),
                 title='Sentiment Distribution', labels={'x':'Sentiment', 'y':'Count'},
                 color=list(sentiment_counts.keys()), color_discrete_map=color_map)
        elif language == 'cy':
            fig_bar = px.bar(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()),
                 title='Dosbarthiad Sentiment', labels={'x':'Sentiment', 'y':'Cyfrif'},
                 color=list(sentiment_counts.keys()), color_discrete_map=color_map)
        fig_bar.update_traces(marker_line_color='black', marker_line_width=0.5)
        fig_bar.write_image("/freetxt/website/static/Sentiment_plots/sentiment_bar.png")
        fig_bar_path = "/freetxt/website/static/Sentiment_plots/sentiment_bar.png"
       # append_logo_to_image(fig_bar_path, logo_path, fig_bar_path)
        fig_bar.write_html("/freetxt/website/static/Sentiment_plots/sentiment_bar.html")
        # Read the existing HTML content
        with open("/freetxt/website/static/Sentiment_plots/sentiment_bar.html", "r", encoding="utf-8") as f:
            content = f.read()

        # Add the "Visualisation by" text and logo image at the bottom
        addition = f"""
    <div style="text-align:center; margin-top:30px;">
        {visualisation_text} <img src="https://ucrel-freetxt-2.lancs.ac.uk/static/images/logo.png" alt="Logo" style="height:40px;">
    </div>
    """

        # Append the new content just before the closing body tag
        content = content.replace("</body>", addition + "\n</body>")

        # Write the updated content back to the file
        with open("/freetxt/website/static/Sentiment_plots/sentiment_bar.html", "w", encoding="utf-8") as f:
            f.write(content)
        plot_html_bar = fig_bar.to_html(full_html=False)
        return data, sentiment_counts, plot_html_pie, plot_html_bar
    return None, None, None

@FileAnalysis.route('/update_sentiment', methods=['POST'])
def handle_sentiment_update():
    data = request.get_json()
    language = data.get('language', 'en')
    #print(data)
    chosen_classes = data.get('sentiment_classes', 3)
    #print(chosen_classes)
    sentences = session.get('mergedData', [])
    
    sentiment_data, sentiment_counts, pie_chart_html, bar_chart_html = sentiment_analysis(sentences,language, chosen_classes)
    session['sentiment_data'] = sentiment_data
    return jsonify({
        "status": "success",
        "sentimentData": sentiment_data,
        "sentimentCounts": sentiment_counts,
        'sentimentPlotPie': pie_chart_html,
        'sentimentPlotBar': bar_chart_html
    })



def summarize_text(text, ratio=0.4):
    return run_summarizer(text, ratio)

@FileAnalysis.route('/summarise', methods=['POST'])
def summarize_route():
    data = request.json
    text = session.get('mergedData', [])
    
    if isinstance(text, list):
        text = ' '.join(text)
    else:
        text=text
    chosen_ratio = data.get('chosen_ratio', 0.4)
    summary = run_summarizer(text, chosen_ratio) 
    #print('Generated summary', summary)
    return jsonify({"summary": summary})
   


@FileAnalysis.route('/clear-session', methods=['POST'])
def clear_session():
    filepath = session.get('uploaded_file_path')
    #print(filepath)
    if filepath and os.path.exists(filepath):
        os.remove(filepath)
        session.pop('uploaded_file_path', None)  # remove the filepath from the session
    session.clear()  # clear other session data if needed
    return jsonify({"message": "Session cleared"})

@FileAnalysis.route('/process_rows', methods=['GET', 'POST'])
def handle_selected_rows():
    data = request.get_json()
    merged_rows = data.get('mergedData', [])
    # print(merged_rows)
    language = data.get('language', 'en')
    session['language']=language
    # Get word frequencies
    words = []
    for sentence in merged_rows:
        extracted_words =  sentence.split()
        filtered_words = [word for word in extracted_words if word not in STOPWORDS ]
        words.extend(filtered_words)

            # Counting frequencies
    word_frequencies = dict(Counter(words))
    # Sorting by highest frequency
    sorted_word_frequencies = dict(sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True))

    # Choose a random word from the list as the search word
    scatter_text_html = None
    sentiment_analyser = SentimentAnalyser()
    # Get sentiment analysis results
    try:
        sentiment_data, sentiment_counts, pie_chart_html, bar_chart_html = sentiment_analysis(merged_rows, language)
        
    except Exception as e:
        print("Error in sentiment analysis:", e)

    try:
        df_sentiment = pd.DataFrame(sentiment_data)
      

        with file_lock:
            scatter_text_html = sentiment_analyser.generate_scattertext_visualization(df_sentiment, language)
           
           
    except Exception as e:
        print("Error in DataFrame or visualization:", e)
    session['scatter_html'] = scatter_text_html
    
    # Word Tree Generator
    sentences = [[s] for s in merged_rows]
    wordTreeData = sentences
    random_word = random.choice(list(sorted_word_frequencies.keys()))
  
    cleaned_word = random_word.translate(str.maketrans('', '', string.punctuation))

    search_word = cleaned_word
    df = pd.DataFrame(merged_rows, columns=['text'])
    analyser = KWICAnalyser(df,language)
        # Initialize the KWICAnalyser with the merged rows
    
    # Get the sorted unique list of semantic tags
    sorted_unique_tags = analyser.get_sorted_unique_tags()
    word_frequencies = analyser.get_word_frequencies()
    unfiltered_word_frequencies = analyser.get_word_frequencies(isUnfiltered=True)
    session['unfiltered_word_frequencies'] = unfiltered_word_frequencies
    session['word_frequencies'] = word_frequencies
    session['mergedData'] = merged_rows
    session['sentiment_data'] = sentiment_data
    summary = summarize_text(merged_rows)

    return jsonify({
        "status": "success",
        "wordFrequencies": word_frequencies,
        "sentimentData": sentiment_data,
        "sentimentCounts": sentiment_counts,
        'sentimentPlotPie': pie_chart_html,
        'sentimentPlotBar': bar_chart_html,
        "wordTreeData": wordTreeData,
        "search_word": search_word,
        "summary": summary, "sortedUniqueTags": sorted_unique_tags, 
        "scatterTextHtml": scatter_text_html
    })
    
@FileAnalysis.route('/get_exampledata_files')
def get_files():
    directory = '/freetxt/website/static/example-data-hub'
    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
    return jsonify(files)


# Cleaning function
def clean_review(review):
    return review.translate(str.maketrans('', '', PUNCS)).lower()





@FileAnalysis.route('/generate_wordcloud', methods=['POST','GET'])
def generate_wordcloud():
    if request.method == 'POST':
        request_data = request.get_json(force=True)
        #print(request.headers.get('Content-Type'))
      
        cloud_shape_path = request_data.get('cloud_shape')
        cloud_outline_color = request_data.get('cloud_outline_color')
        cloud_type = request_data.get('cloud_type')
        cloud_measure = request_data.get('cloud_measure')
#print(cloud_shape_path)
        if cloud_shape_path== '':
                cloud_type = 'all_words'
                #cloud_shape_path = 'https://ucrel-freetxt-2.lancs.ac.uk/static/images/img/cloud.png'
                cloud_shape_path = '/freetxt/website/static/images/img/cloud.png'
                cloud_outline_color= 'white'
                cloud_measure='KENESS'
       
        
        data_json = session.get('mergedData')
        
        input_data = pd.DataFrame(data_json)
       
        # Clean each review in the DataFrame
        input_data[input_data.columns[0]] = input_data[input_data.columns[0]].apply(clean_review)
       
          # Detect the language using the langdetect package
        combined_text = ' '.join(input_data[0].astype(str))
        try:
            language = detect(combined_text)
        except:
            language = 'en'  # default to English if detection fails
        
        cloud_generator = WordCloudGenerator(input_data)
        wc_path, word_list = cloud_generator.generate_wordcloud(cloud_shape_path, cloud_outline_color, cloud_type, language, cloud_measure, wordlist={})
        
        session['word_cloud_src'] = wc_path
        
        # Handles the generation of the second cloud, containing the words with the selected semantic tags
        if cloud_type == 'semantic_tags' and session["tokens_with_semantic_tags"]:
            words_tags = session["tokens_with_semantic_tags"]
            words_with_sem_tags = [word for (word, pos, tag) in words_tags if tag in word_list]
            sec_wc_path, sec_word_list = cloud_generator.generate_wordcloud(cloud_shape_path, cloud_outline_color, 'all_words', language, cloud_measure, words_with_sem_tags, f"{time.time()}_words")
            
            session['sec_word_cloud_src'] = sec_wc_path
            print(sec_wc_path)
            
            return jsonify({
                "status": "success",
                "wordcloud_image_path": [wc_path, sec_wc_path],
                "word_list": [word_list, sec_word_list]
            })
            
        return jsonify({
            "status": "success",
            "wordcloud_image_path": wc_path,
            "word_list": word_list
        })
    return jsonify({"status": "error", "message": "Invalid request method"})


@FileAnalysis.route('/Keyword_collocation', methods=['POST'])
def analyse():
    window_size = request.form.get('window_size', 5, type=int)
    selected_word = request.form.get('word_selection')
    selected_tag = request.form.get('tag_selection')
    selected_semantic = request.form.get('sem_selection')
    kwic_option = request.form.get('kwic_option')
    language = session.get('language')
    data_json = session.get('mergedData')
    input_data = pd.DataFrame(data_json)
    analyzer = KWICAnalyser(input_data,language)

    if kwic_option == "word" and selected_word:
        kwic_results = analyzer.get_kwic(selected_word, window_size)
        collocs_strengths = analyzer.get_collocation_strength(selected_word)
    elif kwic_option == "tag" and selected_tag:
        kwic_results = analyzer.get_kwic(selected_tag, window_size, by_tag=True)
        if not kwic_results:
           #flash("No instances found. Please choose another category as the data does not have this category.")
           return jsonify({"error": "Invalid KWIC option or missing selection!", 'message': 'No instances found. Please choose another category as the data does not have this category.'})
        else:
            collocs_strengths = analyzer.get_collocation_strength(selected_word if selected_word else selected_tag, by_tag=True)

    elif kwic_option == "sem" and selected_semantic:
        kwic_results = analyzer.get_kwic(selected_semantic, window_size, by_sem=True)
        collocs_strengths = analyzer.get_collocation_strength(selected_word if selected_word else selected_semantic, by_sem=True)
    else:
        return jsonify({"error": "Invalid KWIC option or missing selection!"})

    collocs = analyzer.get_collocs(kwic_results)
    combined_collocs = [(colloc, freq, mi, ll) for (colloc, freq) in collocs for _, mi, ll in collocs_strengths if colloc == _]
    combined_collocs.sort(key=lambda x: x[2], reverse=True)

    if kwic_option == "word":
        keyword_for_plot = selected_word
    elif kwic_option == "tag":
        keyword_for_plot = selected_tag
    elif kwic_option == "sem":
        keyword_for_plot = selected_semantic
    else:
        return jsonify({"error": "Invalid KWIC option!"})

    graph_path = analyzer.plot_coll_14(keyword_for_plot, collocs)
    session['keyword_results'] = kwic_results
    session['combined_collocs'] = combined_collocs
    session['graph_path'] = graph_path
    return jsonify({
        "kwic_results": kwic_results,
        "collocs": combined_collocs,
        "graph_path": graph_path
    })

@FileAnalysis.route('/get_file_list')
def get_file_list():
    folder_path = '/freetxt/website/static/example-data-hub'  
    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    return jsonify(files)

@FileAnalysis.route('/generate-pdf', methods=['POST'])
def generate_pdf():
    if request.method == 'POST':
        selected_sections = request.form.getlist('sections') if 'sections' in request.form else []
        #print(selected_sections)
        if 'keyword_results' in session:
            keyword_results = session.get('keyword_results')
            keyword_content = [{'Left Context': item[0], 'Keyword': item[1], 'Right Context': item[2]} for item in keyword_results]
        else:
            keyword_content=[{'Left Context':'please select a keyword first','Keyword':'','Right Context':''}] 
        file_name = session.get('uploaded_file_name', 'No file uploaded')
        
        sentiment_explanation = request.form['sentiment_explanation'] if 'sentiment_explanation' in request.form else ""
        ##request.form['sentimentViewContent'] if 'sentimentViewContent' in request.form else ""
        #SentimentPlot = request.form['SentimentPlot'] if 'SentimentPlot' in request.form else ""
        #SentimentPlotbar = request.form['SentimentPlotbar'] if 'SentimentPlotbar' in request.form else ""
        wordtree=request.form['wordtree'] if 'wordtree' in request.form else ""
        cloudtype = request.form['Cloud_type'] if 'Cloud_type' in request.form else ""
        cloudmeasure = request.form['Cloud_measure'] if 'Cloud_measure' in request.form else ""
        summary=request.form['summaryField'] if 'summaryField' in request.form else ""
        SentimentPlot = 'https://ucrel-freetxt-2.lancs.ac.uk/static/Sentiment_plots/sentiment_pie.png'
        SentimentPlotbar= 'https://ucrel-freetxt-2.lancs.ac.uk/static/Sentiment_plots/sentiment_bar.png'
        word_cloud_image_src = request.form['wordCloudImageSrc'] if 'wordCloudImageSrc' in request.form else ""
        rendered_html = render_template('report.html', 
                                        # Assuming you want to use it in the template
                                       keyword_content=keyword_content, SentimentPlot=SentimentPlot,
                                       SentimentPlotbar=SentimentPlotbar,wordtree=wordtree, sentiment_explanation=sentiment_explanation,
                                       word_cloud_image_src=word_cloud_image_src, summary = summary, selected_sections=selected_sections,
                                      cloudmeasure= cloudmeasure, file_name=file_name,
                                       cloudtype=cloudtype, datetime=datetime)
        # Convert HTML to PDF using WeasyPrint
        pdf = HTML(string=rendered_html).write_pdf()
    
        # Send the generated PDF as a response
        response = Response(pdf, content_type='application/pdf')
        response.headers['Content-Disposition'] = 'inline; filename=report.pdf'
        return response
    else:
        # Handle GET request if needed. 
        return "This endpoint accepts POST requests with form data to generate a PDF."


@FileAnalysis.route('/check_language', methods=['POST'])
def check_language():
    try:
        request_data = request.get_json()
        selected_columns = request_data.get('columns', [])
        language = request_data.get('language', 'en')

        # Extract text data from selected columns
        text_data = []
        for column in selected_columns:
            values = column.get('values', [])
            text_data.extend(values)

        # Create a DataFrame from the extracted text data
        data = pd.DataFrame({'text': text_data})
        # Get the original file name from the session
        full_file_path = session.get('uploaded_file_path')
          # Extract the file name from the full path
        original_file_name = os.path.basename(full_file_path)
        english_data_dict = {}
        welsh_data_dict = {}
        
        checker = LanguageChecker(data, 'text')
        english_data, welsh_data = checker.detect_and_split_languages()
        if english_data is not None:
            english_data_dict['text'] = english_data
        if welsh_data is not None:
            welsh_data_dict['text'] = welsh_data

        has_english = bool(english_data_dict)
        has_welsh = bool(welsh_data_dict)
        
        if has_english and not has_welsh:
            message = f'Your data is only in English from file: {original_file_name}' if language == 'en' else f'Mae eich data yn Saesneg yn unig o\'r ffeil: {original_file_name}.'
            return jsonify({'message': message})

        elif has_welsh and not has_english:
            message = f'Your data is only in Welsh from file: {original_file_name}' if language == 'en' else f'Mae eich data yn Gymraeg yn unigo\'r ffeil: {original_file_name}.'
            return jsonify({'message': message})

        elif has_english and has_welsh:
            # Use the original file name for CSV files
            english_filename = f'{original_file_name}_english.csv'
            welsh_filename = f'{original_file_name}_welsh.csv'
            # Save these dataframes to temporary CSV files and keep track of the paths
            english_path = os.path.abspath(os.path.join('/freetxt/website/static/temp', english_filename))
            welsh_path = os.path.abspath(os.path.join('/freetxt/website/static/temp', welsh_filename))

            pd.concat(english_data_dict.values()).to_csv(english_path, index=False)
            pd.concat(welsh_data_dict.values()).to_csv(welsh_path, index=False)

            session['english_data_path'] = english_path
            session['welsh_data_path'] = welsh_path

            message = 'Mae eich data yn cynnwys Saesneg a Chymraeg.' if language != 'en' else 'Your data contains both English and Welsh.'
            return jsonify({'message': message, 'has_both': True})

        return jsonify({'error': 'Unable to determine the language of your data.'})

    except Exception as e:
        return jsonify({'error': f'An error occurred: {str(e)}'})




@FileAnalysis.route('/download/english')
def download_english():
    english_path = session.get('english_data_path')
    if not english_path:
        return "File not found.", 404
    # Extract the original file name from the path
    original_file_name = os.path.basename(english_path)

    # Set the original file name as the download name
    return send_file(english_path, as_attachment=True, download_name=original_file_name)


@FileAnalysis.route('/download/welsh')
def download_welsh():
    welsh_path = session.get('welsh_data_path')
    if not welsh_path:
        return "File not found.", 404

    original_file_name = os.path.basename(welsh_path)

    # Set the original file name as the download name
    return send_file(welsh_path, as_attachment=True, download_name=original_file_name)


@FileAnalysis.route('/get-html-pie')
def get_html_graph():
    return send_from_directory(directory='static/Sentiment_plots', path='sentiment_pie.html', as_attachment=True)

@FileAnalysis.route('/get-html-bar')
def get_html_bar():
    return send_from_directory(directory='static/Sentiment_plots', path='sentiment_bar.html', as_attachment=True)

@FileAnalysis.route('/get-html-scatter')
def get_html_scatter():
    if 'scatter_html' in session:
        scatter_html = session.get('scatter_html')
        
        # Extracting the base filename and its extension
        base, ext = splitext(basename(scatter_html))
        
        # Appending '_logo' before the file extension
        filename_with_logo = f"{base}_logo{ext}"

        # Sending the file from the directory
        return send_from_directory(directory='static/wordcloud', path=filename_with_logo, as_attachment=True)
    else:
        # Handle the case where 'scatter_html' is not in session
        return "Scatter HTML not found in session", 404



@FileAnalysis.route('/get_image_scatter')
def get_image_scatter():
    if 'scatter_html' in session:
        scatter_html = session.get('scatter_html')
        absolute_html_path = os.path.join(FileAnalysis.root_path, scatter_html.strip('/'))

        if os.path.exists(absolute_html_path):
            # Read and modify the HTML content
            with open(absolute_html_path, 'r', encoding="utf-8") as file:
                html_content = file.read()

            modified_html_content = '<style>body { background: white; }</style>' + html_content

            # Define the path for the modified HTML file
            modified_html_filename = 'modified_scatter.html'
            modified_html_path = os.path.join('/freetxt/website/static/wordcloud', modified_html_filename)

            # Write the modified HTML content to the file
            with open(modified_html_path, 'w', encoding="utf-8") as file:
                file.write(modified_html_content)

            # Convert the modified HTML to an image
            hti = Html2Image()
            output_filename = 'scatter_image.png'
            path = str(FileAnalysis.root_path) + str('/static/wordcloud/')
            output_image_path = os.path.join(path, output_filename)

            hti.output_path = '/freetxt/website/static/wordcloud/'
            try:
    # Attempt to generate and save the image
               hti.screenshot(html_file=modified_html_path, save_as=output_filename)
               print("Image generated successfully")
               print("Image file exists:", output_image_path)
            except Exception as e:
    # If an error occurs, print the error message
               print("Error in generating the image:", e)

            # Return the image file as a downloadable response
            return send_file(output_image_path, output_filename, as_attachment=True)

        else:
            return "Scatter HTML file not found", 404

def add_logo_and_text_to_image(image_path):
    # Open the word cloud image
    wc_image = Image.open(image_path)
    visualisation_text_en = "Visualisation by"
    visualisation_text_cy = "Gweledigaeth gan"
    language = session.get('language')
    print(language)
    if language == 'en':
            visualisation_text = visualisation_text_en
    elif language == 'cy':
             visualisation_text = visualisation_text_cy
    # Open the logo image (update the path to your logo's path)
    logo_image = Image.open("home/khallafn/Freetxt-flask/website/static/images/logo.png")
    logo_image = logo_image.resize((100, 40))  # Resize logo

    # Paste the logo onto the word cloud image
    wc_image.paste(logo_image, (10, wc_image.height - 50), logo_image)

    # Optionally add text
    draw = ImageDraw.Draw(wc_image)
    font = ImageFont.load_default()  # Or specify a custom font
    draw.text((120, wc_image.height - 45),  visualisation_text, (0, 0, 0), font=font)

    # Save the modified image back to the same path
    wc_image.save(image_path)


@FileAnalysis.route('/get-word-cloud')
def get_word_cloudsrc():
    if 'word_cloud_src' in session:
        word_cloud_src = session.get('word_cloud_src')
        filename = os.path.basename(word_cloud_src)
        add_logo_and_text_to_image(os.path.join('/freetxt/website/static/wordcloud/', filename))       
    return send_from_directory(directory='static/wordcloud/', path=filename, as_attachment=True)



@FileAnalysis.route('/get-graph-path')
def get_graph_path():
    if 'graph_path' in session:
        graph_path = session.get('graph_path')
        filename = os.path.basename(graph_path)
            
    return send_from_directory(directory='static/network_graphs/', path=filename, as_attachment=True)

import traceback
from io import BytesIO
@FileAnalysis.route('/get-Sentimentdata')
def get_Sentimentdata():
    try:
        if 'sentiment_data' in session:
            sentimentData = session.get('sentiment_data')
           # print(sentimentData)
            data = pd.DataFrame(sentimentData)
            #print(data)
            # Convert DataFrame to CSV and hold it in memory
            buffer = BytesIO()
            data.to_csv(buffer, index=False, encoding='utf-8')
            buffer.seek(0)  # Return to the start of the bufferr
            
            return Response(
                buffer.getvalue(),
                headers={
                    "Content-Disposition": "attachment; filename=sentimentData.csv",
                    "Content-type": "text/csv"
                }
            )
        else:
            return "No sentiment data found in the session", 400

    except Exception as e:
        # Print the error to the Flask console
        print(traceback.format_exc())
        return "Server encountered an error", 500

@FileAnalysis.route('/get-keyword-data')
def get_keyword_data():
    try:
        if 'keyword_results' in session:
            keywordresults = session.get('keyword_results')
            structured_data = [{
            'Left Context': item[0],
            'Keyword': item[1],
            'Right Context': item[2]
                } for item in keywordresults]
            data = pd.DataFrame(structured_data)
            buffer = BytesIO()
            data.to_csv(buffer, index=False, encoding='utf-8')
            buffer.seek(0)  # Return to the start of the bufferr
            return Response(
                buffer.getvalue(),
                headers={
                 "Content-Disposition": "attachment; filename=keywordData.csv",
                "Content-type": "text/csv"
                }
            )
    except Exception as e:
        # Handle  exception here
        return "Server encountered an error", 500
    
@FileAnalysis.route('/get-word-frequencies-data')
def get_word_frequencies():
    try:
        if 'word_frequencies' in session and 'unfiltered_word_frequencies' in session:
            word_frequencies = session['word_frequencies']
            unfiltered_word_frequencies = session['unfiltered_word_frequencies']
            
            # Sorted by frequency
            sorted_word_frequencies = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item: (-item[1], item[0]))}
            sorted_unfiltered_word_frequencies = {k: v for k, v in sorted(unfiltered_word_frequencies.items(), key=lambda item: (-item[1], item[0]))}
            
            # The filtered data
            structured_data = [{
                'word': key,
                'frequency': val
                } for key, val in sorted_word_frequencies.items()]
            
            # The unfiltered data
            unfiltered_structured_data = [{
                'word': key,
                'frequency': val
                } for key, val in sorted_unfiltered_word_frequencies.items()]
            
            filtered_data = pd.DataFrame(structured_data)
            unfiltered_data = pd.DataFrame(unfiltered_structured_data)
            
            # Create headings and gaps
            unfiltered_heading = pd.DataFrame([["Unfiltered Data", ""]], columns=["word", "frequency"])
            gap = pd.DataFrame([["", ""]], columns=["word", "frequency"])
            
            merged_frequency_data = pd.concat([filtered_data, gap, unfiltered_heading, unfiltered_data]).reset_index(drop=True)
            
            buffer = BytesIO()
            merged_frequency_data.to_csv(buffer, index=False, encoding='utf-8')
            buffer.seek(0)
            
            return Response(
                buffer.getvalue(),
                headers={
                 "Content-Disposition": "attachment; filename=keywordData.csv",
                "Content-type": "text/csv"
                }
            )
    except Exception as e:
        return "Server encountered an error", 500
            
# Function to retain word clusters or individual words
def retain_clusters(text, clusters):
    words = text.split()
    retained_words = [word for word in words if any(cluster in word for cluster in clusters)]
    return ' '.join(retained_words)

@FileAnalysis.route('/regenerate_wordcloud', methods=['POST'])
def regenerate_wordcloud():
    if request.method == 'POST':
        request_data = request.get_json(force=True)
        
        # Get the provided words from the request
        provided_words = set(request_data.get('words', []))
        
        label_name = request_data.get('label')
        cloud_shape_path = request_data.get('cloud_shape')
        cloud_outline_color = request_data.get('cloud_outline_color')
        cloud_type = request_data.get('cloud_type')
        cloud_measure = request_data['cloud_measure']
        if cloud_shape_path == '':
            cloud_type = 'all_words'
            #cloud_shape_path = 'https://ucrel-freetxt-2.lancs.ac.uk/website/static/images/img/cloud.png'
            cloud_shape_path = '/freetxt/website/static/images/img/cloud.png'
            cloud_outline_color= 'white'
            cloud_measure='KENESS'
        # Extract the data and filter based on provided words
        data_json = session.get('mergedData')
        
        input_data = pd.DataFrame(data_json)
       
        # Clean each review in the DataFrame
        input_data[input_data.columns[0]] = input_data[input_data.columns[0]].apply(clean_review)
       
        # Detect the language using the langdetect package
        combined_text = ' '.join(input_data[0].astype(str))
        try:
            language = detect(combined_text)
        except:
            language = 'en'  # default to English if detection fails
        
        cloud_generator = WordCloudGenerator(input_data)
        wc_path, word_list = cloud_generator.generate_wordcloud(cloud_shape_path, cloud_outline_color, cloud_type, language ,cloud_measure, provided_words)
       
        session['word_cloud_src'] = wc_path
        
        return jsonify({
            "status": "success",
            "wordcloud_image_path": wc_path,
            "word_list": word_list
        })

    return jsonify({"status": "error", "message": "Invalid request method"})

@FileAnalysis.route('/update_graph', methods=['POST'])
def handle_graph_update():
    data = request.get_json()
    keyword = data.get('keyword') 
    collocs = data.get('collocs', [])
    graph_type = data.get('graphType', 'frequency')
    
    # Call the function to update the graph and get the graph path
    graph_path = KWICAnalyser.update_graph(keyword, collocs, graph_type)
    
    session['graph_path'] = graph_path
    # Return the graph path to the client
    return jsonify({
        "status": "success",
        "graphPath": graph_path
    })
@FileAnalysis.route('/get-collos-data')
def get_collos_data():
    try:
        if 'combined_collocs' in session:
            combined_collocs = session.get('combined_collocs')
            buffer = StringIO()
            structured_data = [{
            'Word': item[0],
            'Frequency': item[1],
            'Mutual Information': item[2], 'Log Liklihood': item[3]
                } for item in combined_collocs]
            data = pd.DataFrame(structured_data)
            buffer = BytesIO()
            data.to_csv(buffer, index=False, encoding='utf-8')
            buffer.seek(0)  # Return to the start of the bufferr
            return Response(
                buffer.getvalue(),
                headers={
                 "Content-Disposition": "attachment; filename=combined_collocsData.csv",
                "Content-type": "text/csv"
                }
            )
    except Exception as e:
       
        return "Server encountered an error", 500